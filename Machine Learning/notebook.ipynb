{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "X = pd.read_csv(\"../Cleaning/new_clean_data.csv\", index_col=0)\n",
    "\n",
    "X.dropna(axis=0, subset=[\"price\"], inplace=True)\n",
    "y = X[\"price\"]\n",
    "X.drop([\"price\"], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting categorical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    cname for cname in X.columns if X[cname].dtype == \"object\"\n",
    "]\n",
    "categorical_cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting numeric values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    cname\n",
    "    for cname in X.columns\n",
    "    if X[cname].dtype in [\"int64\", \"float64\"]\n",
    "]\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "X[\"age\"] = now.year - X[\"year\"] + 1\n",
    "X[\"mileage_over_year\"] = X[\"mileage\"] + X[\"age\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "data=pd.concat([X,y], axis=1)\n",
    "for col in categorical_cols:\n",
    "    csq=chi2_contingency(pd.crosstab(data['price'], data[col]))\n",
    "    print(f\"P-value (price, {col}): \",csq[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brand', 'model', 'location', 'body_type', 'fuel', 'transmission']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical_cols=categorical_cols[:-1]\n",
    "categorical_cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into Train, Validation and Test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=0\n",
    ")\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "    X_train_full, y_train, test_size=0.25, random_state=0\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep selected columns only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_cols =  categorical_cols+ numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer for num data\n",
    "numerical_transformer = SimpleImputer(strategy=\"mean\")\n",
    "imputed_num_train = pd.DataFrame(numerical_transformer.fit_transform(X_train[numerical_cols]))\n",
    "\n",
    "imputed_num_valid = pd.DataFrame(numerical_transformer.transform(X_valid[numerical_cols]))\n",
    "#imputer for cat data\n",
    "categorical_transformer = SimpleImputer(strategy=\"most_frequent\")\n",
    "imputed_cat_train = pd.DataFrame(categorical_transformer.fit_transform(X_train[categorical_cols]))\n",
    "\n",
    "imputed_cat_valid = pd.DataFrame(categorical_transformer.transform(X_valid[categorical_cols]))\n",
    "\n",
    "imputed_num_train.columns=numerical_cols\n",
    "imputed_num_valid.columns=numerical_cols\n",
    "imputed_cat_train.columns=categorical_cols\n",
    "imputed_cat_valid.columns=categorical_cols\n",
    "\n",
    "X_train = pd.concat([imputed_num_train, imputed_cat_train], axis=1)\n",
    "X_valid = pd.concat([imputed_num_valid, imputed_cat_valid], axis=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_cols]))\n",
    "OH_X_valid = pd.DataFrame(OH_encoder.transform(X_valid[categorical_cols]))\n",
    "OH_X_train.index = X_train.index\n",
    "OH_X_valid.index = X_valid.index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_X_train = X_train.drop(categorical_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(categorical_cols, axis=1)\n",
    "X_train = pd.concat([num_X_train, OH_X_train], axis=1)\n",
    "X_valid = pd.concat([num_X_valid, OH_X_valid], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=7)\n",
    "forest_model.fit(X_train, y_train)\n",
    "preds = forest_model.predict(X_valid)\n",
    "print(r2_score(y_valid, preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the random grid to search for best hyperparameters\n",
    "# # First create the base model to tune\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# rf = RandomForestRegressor()\n",
    "# # Random search of parameters, using 3 fold cross validation, \n",
    "# # search across 100 different combinations, and use all available cores\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n",
    "# # Fit the random search model\n",
    "# rf_random.fit(X_train, y_train)\n",
    "# rf_random.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_rf_params={'n_estimators': 800,\n",
    " 'min_samples_split': 2,\n",
    " 'min_samples_leaf': 1,\n",
    " 'max_features': 'auto',\n",
    " 'max_depth': 100,\n",
    " 'bootstrap': True}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [ 400,600, 800, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 2, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6097806160261633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_cv_param={'bootstrap': True,\n",
    " 'max_depth': 110,\n",
    " 'max_features': 3,\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 8,\n",
    " 'n_estimators': 600}\n",
    "forest_model = RandomForestRegressor(n_estimators=400,\n",
    " min_samples_split= 12,\n",
    " min_samples_leaf= 1,\n",
    " max_features= 10,\n",
    " max_depth= 110,\n",
    ")\n",
    "forest_model.fit(X_train, y_train)\n",
    "preds = forest_model.predict(X_valid)\n",
    "print(r2_score(y_valid, preds))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(\n",
    "    n_estimators=1000, early_stopping_rounds=5, learning_rate=0.05, n_jobs=4\n",
    ")\n",
    "xgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb.predict(X_valid)\n",
    "acc = r2_score(y_valid, predictions)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# xgb1 = XGBRegressor()\n",
    "# parameters = {'nthread':[4], \n",
    "#               'objective':['reg:linear'],\n",
    "#               'learning_rate': [.03, 0.05, .07], \n",
    "#               'max_depth': [5, 6, 7],\n",
    "#               'min_child_weight': [4],\n",
    "#               'subsample': [0.7],\n",
    "#               'colsample_bytree': [0.7],\n",
    "#               'n_estimators': [500]}\n",
    "\n",
    "# xgb_grid = GridSearchCV(xgb1,\n",
    "#                         parameters,\n",
    "#                         cv = 2,\n",
    "#                         n_jobs = 5,\n",
    "#                         verbose=True)\n",
    "\n",
    "# xgb_grid.fit(X_train,\n",
    "#          y_train)\n",
    "\n",
    "# print(xgb_grid.best_score_)\n",
    "# print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_param={'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, 'n_estimators': 500, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}\n",
    "xgb = XGBRegressor(\n",
    "colsample_bytree= 0.7, learning_rate= 0.03, max_depth= 5, min_child_weight= 4, n_estimators= 500, nthread= 4, objective= 'reg:linear',  subsample= 0.7)\n",
    "xgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb.predict(X_valid)\n",
    "acc = r2_score(y_valid, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, \n",
    "                    10.0, 20, 50, 100, 500, 1000 ]}\n",
    "\n",
    "#initialising Ridge() function\n",
    "ridge = Ridge()\n",
    "# defining cross validation folds as 5\n",
    "folds = 5\n",
    "# Defining GridSearchCV\n",
    "grid_cv_model = GridSearchCV(estimator=ridge,\n",
    "                       param_grid=params,\n",
    "                       \n",
    "                       cv=folds,\n",
    "                       return_train_score=True,\n",
    "                       verbose=1)\n",
    "grid_cv_model.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_cv_model.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "# Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
    "params = {'max_depth':[1,2, 3,5, 10, 15, 16, 17,20]}\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "grid = GridSearchCV(estimator=dt, param_grid=params, cv=3)\n",
    "\n",
    "tree_reg = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "grid.best_score_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "parameters = {'fit_intercept':[False], 'normalize':[False], 'copy_X':[True, False]}\n",
    "gs = GridSearchCV(estimator=model, param_grid=parameters, cv=3, n_jobs=-1, verbose=1, return_train_score=True)\n",
    "lin_reg = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
